---
title: "Inference stack"
description: "Display inline code and code blocks"
icon: "code"
---

## Model flavors

When it comes to inference, there is a lot of different trade off for us a provider:

- Batch size
- GPU model: L40, H100, H200, B200
- GPU count

To show possible options we do two flavors:

- Fast
- Base

They differ in price per token and computation speed.

The fast flavor has a lower batch size and higher computational resources, resulting in faster output speeds. Model quality remains consistent across both options.

To access the fast flavor, add `-fast` to the model name when calling it in the [API](https://docs.nebius.com/studio/inference/api).

## Inference optimisations

Our LLM inference service employs a range of optimization techniques to increase throughput while maintaining model quality. These techniques include:

1. KV cache: A caching mechanism that stores frequently accessed key-value pairs, reducing the number of computations required.
2. Paged attention: A technique that divides the input sequence into smaller chunks, processing each chunk separately to reduce memory usage and computation.
3. Flash attention: A modified attention mechanism that reduces the number of computations required for attention calculations.
4. Quantization: A technique that reduces the precision of model weights and activations, decreasing memory usage and computation.
5. Continuous batching: A method that batches multiple input sequences together, increasing throughput by reducing the overhead of individual requests.
6. Context caching: A caching mechanism that stores the context (i.e., the output of previous layers) for each input sequence, reducing the number of computations required.
7. Speculative decoding: For fast flavors # TODO

### **Impact on model quality**

Our optimization techniques are designed to minimize the impact on model quality. Through extensive testing and evaluation, we have found that our optimized models maintain approximately 99% of the original model's quality.

This means that the optimized models produce nearly identical results to the original models, with only minor differences in output.The quality impact of each optimization technique is carefully evaluated and monitored to ensure that the cumulative effect of all techniques does not compromise the overall quality of the model. Our goal is to provide a high-throughput inference service that delivers accurate and reliable results, while minimizing the computational resources required.