---
description: "Description of your new file."
title: "Playground"
---

Nebius AI Studio provides a _model playground_ where users can test various AI models through a web interface without coding.

## **How to use the playground**

1. On the [Models](https://studio.nebius.com/) page, choose a model and click  **Go to playground** on its card.
2. Under **System prompt**, enter instructions that the model should follow when generating output. You can include examples of good in-context learning outputs (few-shot prompting). For example:

   > Add a fun fact about trains to the end of each response. The fun fact may not be related to what I'm asking you about. Here is an example:
   > - User: What is the capital of Croatia?\\
   > - You: The capital of Croatia is Zagreb. By the way, did you know that the first railroad in New England was powered by horses?
3. Start chatting to the model.

After testing a model setup, you can save it in one of the following ways:

- Copy your model setup and chat as code to use in your application. To do this, click **View code** on the left panel. For more details, see [View code](https://docs.nebius.com/studio/inference/playground#view-code).
- Save your model setup as a prompt preset. To do this, click **Save preset** on the left panel. For more details, see [Prompt presets in Nebius AI Studio](https://docs.nebius.com/studio/prompt-presets).

## **Model card**

Model card in Nebius AI Studio UI highlights important parameters for each model:

- Input tokens. Price per token included in the API request, in USD per million tokens.
- Output tokens. Price per token generated by the model (and received from the API), in USD per million tokens.
- Tokens per sec. Tokens receive speed when the model generates them. The estimated speed is approximate and may vary.
- Quality. [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) score according to the benchmarking claimed by the model provider.

To get an extended list of model parameters, send a request that returns a [list of models](https://docs.nebius.com/studio/api/examples#models). Specify the `verbose=true` query parameter in the request endpoint.

## **Model parameters**

The playground allows you to set basic sampling parameters for the model you have chosen.

**Note**

For more parameters supported by the [inference API](https://studio.nebius.com/api-reference), refer to the [vLLM documentation](https://docs.vllm.ai/en/stable/dev/sampling_params.html#vllm.SamplingParams).

### **Temperature**

- **Affects**: Output randomness
- **Models**: All
- **Type**: Number
- **Range of values**: From 0 to 1 for meta-llama/Meta-Llama-3, from 0 to 2 for models in other families
- **Default value**: From 0.3 to 0.7, depending on the model

Temperature determines how "hot-headed" the model predictions are. The higher the temperature, the more random and less deterministic and conservative the output.

> For example, a 0.8 temperature makes outputs more creative and random than a 0.5.

### **Maximum tokens**

- **Affects**: Computational cost
- **Models**: All
- **Type**: Number
- **Range of values**: Depends on the model
- **Default value**: Depends on the model

The maximum number of tokens that the model generates. If set to 0, the number of tokens is unlimited.

### **Presence penalty**

- **Affects**: Output randomness
- **Models**: All
- **Type**: Number
- **Range of values**: From −2 to 2
- **Default value**: 0

The presence penalty is applied to new tokens that have previously appeared in the output. Positive values penalize such tokens and negative values favor them.

### **Top-p threshold**

- **Affects**: Output randomness
- **Models**: All
- **Type**: Number
- **Range of values**: From 0 to 1
- **Default value**: From 0.9 to 1, depending on the model

In _top-p sampling_, also known as _nucleus sampling_, the model considers only the most probable tokens whose combined probability mass is equal to the specified threshold.

> For example, with a threshold of 0.1, only the tokens that comprise the top 10% of the probability mass are considered.

To consider all tokens, set the threshold to 1.

If both top-p and top-k thresholds are specified, top-k sampling will be applied first.

### **Top-k threshold**

- **Affects**: Output randomness
- **Models**: All
- **Type**: Number
- **Range of values**: From 1 to 200
- **Default value**: 50

In _top-k sampling_, only the _k_ most probable tokens are considered.

> For example, with a top-k threshold of `100`, the model will only consider the top 100 most probable tokens at each step of text generation.

To consider all tokens, set the threshold to 0.

If both top-k and top-p thresholds are specified, top-k sampling will be applied first.

### **Stop sequence**

- **Affects**: Computational cost
- **Models**: All
- **Type**: Array of strings

When the stop sequence appears in the output, it is excluded. The model then stops generating tokens. Up to 4 stop sequences are supported.

### **How to view more model parameters**

To get an extended list of model parameters, send a request that returns a [list of models](https://docs.nebius.com/studio/api/examples#models). Specify the `verbose=true` query parameter in the request endpoint.

## **LoRA adapters for text-to-image models**

When you work with a [text-to-image model](https://docs.nebius.com/studio/inference/models/text-to-image) in the playground, you can add LoRA adapters to the model and generate images in a consistent style. For example, you can receive images with similar characters in the same setting.

For more information, see [Generating images in a consistent style by using LoRA adapters](https://docs.nebius.com/studio/lora/images).

### **Link to LoRA adapter**

Specify the download link to the LoRA adapter file from [Civitai](https://civitai.com/models). Nebius AI Studio extracts the LoRA adapter from the provided download link. For more information, see [Get the link to the LoRA adapter](https://docs.nebius.com/studio/lora/images#get-link).

- **Affects**: Style of the generated image
- **Models**: Text-to-image
- **Type**: String

### **Scale**

The scale of the LoRA adapter impacts how strongly the image style applies. For example, the style is more pronounced in images if the scale is `3` and less pronounced if the scale is `0.5`.

To change the scale, click  in the **Link to LoRA adapter** line.

- **Affects**: Style of the generated image
- **Models**: Text-to-image
- **Type**: Float
- **Range of values**: From 0 to 4
- **Default value**: 1

## **Compare setups**

To work with two model setups side by side, click **Compare**. This will open two chat windows where you can submit inputs for both setups at the same time.

You can also synchronize model parameters and system prompts between the two chats. To do this, click **Copy to right** or **Copy to left** in the **Parameters** menu or next to a system prompt.

**Note**

If the models have different sets of parameter, synchronization may not be possible.

To go back to the single-model mode, close one of the chats. The remaining chat will keep its setup and history.

## **View code**

You can copy your model setup and chat as code to continue working with them in your application:

- Single-model mode: Next to **Setup**, click **View code**.
- Compare mode: In a chat window, click → **View code**.