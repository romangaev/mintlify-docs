---
title: "Embedding Models"
---

To turn your text inputs into vectors of numbers (embeddings), use the [LlamaIndex integration](https://docs.llamaindex.ai/en/latest/examples/embeddings/nebius/) with [embedding models](https://docs.nebius.com/studio/inference/models/embedding).

## **Prerequisites**

1. [Create an API key](https://docs.nebius.com/studio/api/authentication) for authentication.
2. Save the API key to an environment variable:

   ```
   export NEBIUS_API_KEY=<API_key>
   ```
3. Install LlamaIndex packages:

   ```
   pip3 install llama-index-embeddings-nebius llama-index
   ```

   <Expandable title="If you get error: externally_managed_ environment">
     Create a [virtual Python environment](https://docs.python.org/3/library/venv.html). You can install packages there that are isolated from the basic environment.

     To prepare a virtual environment:
     1. Create it:

        ```
        python3 -m venv <environment_name>
        ```
     2. Activate the environment:

        ```
        source <environment_name>/bin/activate
        ```

        A directory with the environment name is created.

     Now, you can install required Python packages. When you no longer need the created virtual environment, run the `deactivate` command and delete the environment directory.
   </Expandable>

## **Prepare a script**

1. Copy the following part of the script:

   ```python
   from llama_index.embeddings.nebius import NebiusEmbedding
   import os
   
   # Take the API key from the environment variable
   NEBIUS_API_KEY = os.getenv("NEBIUS_API_KEY")
   
   # Load an AI model
   embed_model = NebiusEmbedding(api_key=NEBIUS_API_KEY)
   ```
2. Add one of the following methods, depending on your use case:

   | **Use case**                               | **Description**                                                                                | **How to implement**                                                                                                                                                        |
   | ------------------------------------------ | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
   | Regular request                            | Get a vector for a text.                                                                       | Call the `embed_model.get_text_embedding(<text>)` method and print its output as `print(embeddings[:5], sep="\n").`                                                         |
   | Asynchronous request                       | Get a vector asynchronously, so methods that follow your request do not wait for the response. | Call the `await embed_model.aget_text_embedding(<text>)` method.                                                                                                            |
   | Batch of texts                             | Get vectors for several texts.                                                                 | Call the `embed_model.get_text_embedding_batch(texts)` method where `texts` is an array of strings. Next, print vectors as `print(*[x[:3] for x in embeddings], sep="\n").` |
   | Asynchronous request with a batch of texts | Get vectors for several texts asynchronously.                                                  | Call the `await embed_model.aget_text_embedding_batch(texts)` method where `texts` is an array of strings.                                                                  |

## **Examples**

### **Asynchronous request**

To get a vector asynchronously, run the script below:

```python
from llama_index.embeddings.nebius import NebiusEmbedding
import os
import asyncio

# Take the API key from the environment variable
NEBIUS_API_KEY = os.getenv("NEBIUS_API_KEY")

# Load an AI model
embed_model = NebiusEmbedding(api_key=NEBIUS_API_KEY)

# Request a vector for the specified text
text = "Everyone loves justice at another person's expense"
async def complete():
    embeddings = await embed_model.aget_text_embedding(text)
    assert len(embeddings) == 4096
    print(len(embeddings), embeddings[:5], sep="\n")
asyncio.run(complete())
```

The output is the following:

```
4096
[-0.0024051666259765625, 0.0083770751953125, -0.005413055419921875, 0.007396697998046875, -0.022247314453125]
```

### **Request with a batch of texts**

To send a batch of texts and receive a vector for every text in this batch, run the following script:

```python
from llama_index.embeddings.nebius import NebiusEmbedding

import os

# Take the API key from the environment variable
NEBIUS_API_KEY = os.getenv("NEBIUS_API_KEY")

# Load an AI model
embed_model = NebiusEmbedding(api_key=NEBIUS_API_KEY)

# Create a batch of texts
texts = [
    "As the hours pass",
    "I will let you know",
    "That I need to ask",
    "Before I'm alone",
]

# Request a vector for the batch
embeddings = embed_model.get_text_embedding_batch(texts)
assert len(embeddings) == 4
assert len(embeddings[0]) == 4096
print(*[x[:3] for x in embeddings], sep="\n")
```

The output is the following:

```
[-0.0003848075866699219, 0.0004799365997314453, 0.011199951171875]
[-0.0037078857421875, 0.0114288330078125, 0.00878143310546875]
[0.005924224853515625, 0.005153656005859375, 0.001438140869140625]
[-0.009490966796875, -0.004852294921875, 0.004779815673828125]
```